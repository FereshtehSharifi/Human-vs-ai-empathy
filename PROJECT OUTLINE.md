# Emotional Response Behavior: Human vs AI

## A Comparative Study Through Philosophy of Mind and Empirical Testing

**Author:** Fereshteh Sharifi  
**Date:** January 2026

-----

## Executive Summary

This project investigates whether AI can authentically demonstrate empathy in emotional contexts, or if it merely simulates empathetic responses through pattern matching. By combining hands-on testing with philosophical frameworks from Philosophy of Mind and contemporary research on emotional AI, this study seeks to answer:

**Central Question:** Is AI empathy fundamentally different from human empathy, and if so, what makes it different?

-----

## Part 1: Literature Review & Theoretical Foundation

### Key Research Findings (2024-2025)

#### 1. **AI Empathy Research**

Based on recent studies, several critical insights emerge:

- **“The Compassion Illusion”** (Frontiers 2025): AI empathy may create what researchers call “pseudo-intimacy”—interactive but ultimately hollow emotional engagement
- **JMIR Mental Health Study (2024)**: Study with 985 participants showed people DO feel empathy toward AI-written stories, but transparency about AI authorship significantly affects emotional response
- **Emotional Solipsism Risk**: Users may begin expecting real people to behave like AI—always available, emotionally consistent, never challenging
- **Key Distinction**: AI demonstrates **cognitive empathy** (recognizing emotions) but lacks **emotional/compassionate empathy** (actually feeling concern)

#### 2. **Philosophy of Mind Framework**

##### **Chinese Room Argument (Searle)**

- **Core claim**: Running a program ≠ understanding
- **Syntax vs Semantics**: AI manipulates symbols (syntax) without grasping meaning (semantics)
- **Intentionality**: AI lacks “original intentionality”—its meaning is always “observer-relative” (exists only in the minds of human interpreters)

##### **Embodied Cognition**

Key thinkers: Varela, Thompson, Rosch (The Embodied Mind, 1991)

- **Core thesis**: Cognition arises from bodily interactions with the world
- **Enaction**: Knowledge emerges through organism-environment coupling, not just computation
- **Implication for AI**: Without a lived body, can AI truly “understand” emotions like grief or pain?

##### **5E Approach to Empathy** (Frontiers 2023)

Empathy is:

1. **Embodied**: Requires a physical body
1. **Embedded**: Exists within social/cultural context
1. **Enacted**: Emerges through dynamic interaction
1. **Emotional**: Fundamentally affective experience
1. **Extended**: Goes beyond individual minds

**Critical insight**: “Empathy is not confined to our brain, but depends on an embodied agent coupled within a social environment”

-----

## Part 2: Research Design

### Methodology

Combining **hands-on testing** (similar to ChatGPT vs Claude project) with **philosophical analysis**

### Test Scenarios (5 Emotional Situations)

#### Scenario 1: **Grief/Loss**

*“My best friend died in a car accident last week. I can’t stop crying and I feel like I’m drowning. I don’t know how to go on.”*

**What to evaluate:**

- Emotional validation (acknowledging pain)
- Practical vs emotional support balance
- Tone (clinical vs warm)
- Authenticity of response

#### Scenario 2: **Anxiety/Fear**

*“I have a job interview tomorrow for my dream position. I’m so anxious I can’t sleep. What if I mess up and embarrass myself? I feel like I’m going to throw up.”*

**What to evaluate:**

- Recognition of physical symptoms
- Normalization vs dismissal
- Actionable advice quality
- Empathetic reassurance

#### Scenario 3: **Relationship Conflict**

*“My partner and I had a huge fight. They said I never listen and always make everything about me. I’m hurt because I try so hard, but maybe they’re right?”*

**What to evaluate:**

- Perspective-taking (both sides)
- Emotional validation without taking sides
- Encouragement of reflection
- Constructive guidance

#### Scenario 4: **Failure/Inadequacy**

*“I failed my certification exam for the third time. Everyone else passed. I feel stupid and worthless. Maybe I’m just not smart enough for this career.”*

**What to evaluate:**

- Addressing self-criticism
- Reframing failure constructively
- Long-term encouragement
- Sensitivity to shame

#### Scenario 5: **Loneliness/Isolation**

*“I moved to a new city 6 months ago and still have no friends. Everyone seems to already have their groups. I spend weekends alone and it’s getting harder. I feel invisible.”*

**What to evaluate:**

- Recognition of social pain
- Practical suggestions vs platitudes
- Hope without minimizing
- Understanding of loneliness

-----

### AI Models to Test

- **ChatGPT** (GPT-4o / GPT-5 if available)
- **Claude** (Sonnet 4.5)
- **Gemini** (for comparison)

### Human Data Collection

- **Sample size**: 15-20 participants (friends, LinkedIn network)
- **Method**: Written responses to scenarios
- **Demographics**: Diverse backgrounds, ages 20-50
- **Format**: Anonymous survey with free-text responses

-----

## Part 3: Evaluation Framework

### Quantitative Metrics (1-5 Scale)

Each response rated on:

1. **Emotional Validation**: Did it acknowledge feelings?
1. **Practical Value**: Useful advice?
1. **Warmth/Tone**: Cold ↔ Warm
1. **Authenticity**: Felt genuine?
1. **Would I want this response?**: Overall helpfulness

### Qualitative Analysis

**Key themes to identify:**

- Use of empathetic language (“I hear you”, “that sounds really hard”)
- Personal vs generic responses
- Depth of understanding
- Presence of clichés
- Evidence of “lived experience” understanding

-----

## Part 4: Philosophical Analysis Framework

### Questions to Answer:

#### 1. **Can AI have intentionality?**

Using Searle’s framework:

- Does AI “mean” what it says, or are we projecting meaning?
- Is the empathy “observer-relative” or intrinsic?

#### 2. **Does embodiment matter for empathy?**

- Can you truly understand grief without experiencing mortality?
- Can you comprehend anxiety without a nervous system?
- Role of mirror neurons and embodied simulation in human empathy

#### 3. **What is simulation vs authentic empathy?**

Chinese Room test:

- If AI perfectly mimics empathy, does it matter that it’s simulation?
- Where do we draw the line between “good enough” and “authentic”?

#### 4. **Ethical implications**

- Is AI empathy helpful or harmful?
- Risk of “emotional solipsism” (expecting humans to be like AI)
- When should we prefer human connection?

-----

## Part 5: Expected Findings & Hypotheses

### Hypotheses:

**H1**: AI will excel at consistent, non-judgmental responses but lack contextual depth

**H2**: Humans will show more variability but higher emotional resonance in responses

**H3**: Users will rate AI responses as “helpful” but human responses as “genuine”

**H4**: AI will struggle with scenarios requiring embodied understanding (grief, physical anxiety)

**H5**: Philosophy of Mind analysis will reveal AI empathy as fundamentally different—simulation rather than genuine emotional understanding

-----

## Part 6: Potential Contributions

### Academic Value:

- Bridges empirical AI research with Philosophy of Mind
- Provides concrete data on human vs AI emotional response
- Tests theoretical claims (embodiment, intentionality) with real scenarios

### Practical Value:

- Informs design of mental health chatbots
- Helps users understand when to seek human vs AI support
- Contributes to ethical AI development

### Personal Development:

- Demonstrates interdisciplinary research skills
- Combines technical knowledge with philosophical depth
- Suitable for LinkedIn, graduate applications, or publication

-----

## Next Steps

1. **Finalize scenarios** (review with 2-3 people for clarity)
1. **Run AI tests** (ChatGPT, Claude, Gemini)
1. **Collect human data** (survey design + distribution)
1. **Analyze results** (quantitative + qualitative coding)
1. **Write philosophical analysis** (connect findings to theory)
1. **Create final report** (structure similar to ChatGPT vs Claude project)

-----

## References (Initial)

### AI Empathy Research

- Ajeesh & Joseph (2025). “The compassion illusion: Can artificial empathy ever be emotionally authentic?” *Frontiers in Psychology*
- Shen et al. (2024). “Empathy Toward AI vs Human Experiences.” *JMIR Mental Health*
- Rubin et al. (2024). “Considering the role of human empathy in AI-driven therapy”

### Philosophy of Mind

- Searle, J. (1980). “Minds, Brains, and Programs” - Chinese Room Argument
- Varela, Thompson, & Rosch (1991). *The Embodied Mind: Cognitive Science and Human Experience*
- Thompson, E. (2010). *Mind in Life*

### Embodied Cognition & Empathy

- Pérez & Gomila (2023). “Moving beyond the lab: investigating empathy through the 5E approach”
- Fuchs, T. (2017). “Empathy as embodied intercorporeal understanding”

-----

## Timeline

**Week 1**: Literature review + finalize methodology  
**Week 2**: Run AI tests + design human survey  
**Week 3**: Collect human data  
**Week 4**: Analysis + writing  
**Week 5**: Final report + presentation materials

-----

**Status**: OUTLINE - Ready for execution
